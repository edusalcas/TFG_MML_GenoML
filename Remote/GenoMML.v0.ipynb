{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "All imports needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fromGenoToMLData as fm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, log_loss, roc_auc_score, confusion_matrix, cohen_kappa_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GenModels_1SmE function\n",
    "Function that obtains the level 1 expert of one repository in Meta Learning 1SmE\n",
    "\n",
    "Parameters:\n",
    "\n",
    "&emsp;__workPath__: String with your workpath<br>\n",
    "&emsp;__handlerMLdata__: handler that contains the *.dataForML files with the data<br>\n",
    "&emsp;__k__: Number of folds generated from the repo<br>\n",
    "\n",
    "Return value:\n",
    "\n",
    "&emsp;A dictionary with the algorithm name and the best model for this algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genModels_1SmE(workPath,\n",
    "                   handlerMLdata,\n",
    "                   k):\n",
    "    ## Initialize variables\n",
    "    \n",
    "    # Here we chose the algorithms we will use (in future this will be a argument)\n",
    "    algsML = [RandomForestClassifier(), \n",
    "              LogisticRegression(solver='lbfgs'), \n",
    "              DecisionTreeClassifier()]\n",
    "    \n",
    "    # These will be the columns for our final data table\n",
    "    algs_cols = [\"Fold\", \n",
    "                 \"Algorithm\", \n",
    "                 \"AUC_Percent\", \n",
    "                 \"Accuracy_Percent\", \n",
    "                 \"Balanced_Accuracy_Percent\", \n",
    "                 \"Log_Loss\", \"Sensitivity\", \n",
    "                 \"Specificity\", \n",
    "                 \"PPV\", \n",
    "                 \"NPV\", \n",
    "                 \"Runtime_Seconds\"]\n",
    "\n",
    "    algs_table = pd.DataFrame(columns=algs_cols)\n",
    "\n",
    "    # These will be the columns for our total data table (same columns as before)\n",
    "    folds_cols =  algs_cols\n",
    "    folds_table = pd.DataFrame(columns=folds_cols)\n",
    "\n",
    "    models = {}      # A dictionary with all models\n",
    "    finalModels = {} # A dictionary with final models\n",
    "\n",
    "    for i in range(1, k + 1):\n",
    "        models[i] = {}\n",
    "    \n",
    "    # Prepare data for training\n",
    "    X = pd.read_csv(handlerMLdata[\"train1mldata\"], header=0, delim_whitespace=True) # Read data\n",
    "    X = X.loc[X[\"AGE\"] != -9] # Delete data with invalid age\n",
    "    X.dropna(inplace=True) # Delete all NaN values\n",
    "    X.drop(\"ID\", 1, inplace=True) # We dont need ID for the training\n",
    "\n",
    "\n",
    "    # Get the target variable\n",
    "    Y = X[\"PHENO\"]\n",
    "    X.drop(\"PHENO\", 1, inplace=True)\n",
    "\n",
    "    # Process it\n",
    "    Y.columns = [\"PHENO\"]\n",
    "    Y = Y.to_numpy()\n",
    "    le = LabelEncoder()\n",
    "    le.fit(Y)\n",
    "    Y_le = le.transform(Y)\n",
    "\n",
    "    X_np = X.to_numpy() # We need our data in numpy format for training (it is in pandas originally)\n",
    "\n",
    "    # Make folds for data training\n",
    "    np.random.seed(1234)\n",
    "    myfolds = KFold(n_splits=k)\n",
    "\n",
    "    for alg in algsML: # For each algorithm\n",
    "\n",
    "        # Get algorithm name and print it\n",
    "        name = alg.__class__.__name__ \n",
    "\n",
    "        print()\n",
    "        print(\"#\" * 30)\n",
    "        print(name + \"\\n\")\n",
    "\n",
    "        fold = 1\n",
    "\n",
    "        for train_index, test_index in myfolds.split(X_np): # For each fold\n",
    "\n",
    "            print()\n",
    "            print(\"\\t\" + \"#\" * 26)\n",
    "            print(\"\\t\" + \"Fold \" + str(fold) + \"\\n\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "\n",
    "            # Get the train and test sets\n",
    "            X_train, X_test = X_np[train_index], X_np[test_index]\n",
    "            Y_train, Y_test = Y_le[train_index], Y_le[test_index]\n",
    "            \n",
    "            # Train\n",
    "            np.random.seed(1234)\n",
    "            alg.fit(X_train, Y_train)\n",
    "\n",
    "            # Save the model\n",
    "            models[fold][name] = alg\n",
    "\n",
    "            test_predictions = alg.predict_proba(X_test)\n",
    "            rocauc = roc_auc_score(Y_test, test_predictions[:, 1])\n",
    "            print(\"\\t\\tAUC: {:.4%}\".format(rocauc))\n",
    "\n",
    "            test_predictions = alg.predict(X_test)\n",
    "            acc = accuracy_score(Y_test, test_predictions)\n",
    "            print(\"\\t\\tAccuracy: {:.4%}\".format(acc))\n",
    "\n",
    "            test_predictions = alg.predict(X_test)\n",
    "            balacc = balanced_accuracy_score(Y_test, test_predictions)\n",
    "            print(\"\\t\\tBalanced Accuracy: {:.4%}\".format(balacc))\n",
    "\n",
    "            test_predictions = alg.predict(X_test)\n",
    "            kappa = cohen_kappa_score(Y_test, test_predictions)\n",
    "            print(\"\\t\\tKappa: {:.4%}\".format(kappa))\n",
    "\n",
    "            CM = confusion_matrix(Y_test, test_predictions)\n",
    "            TN = CM[0][0]\n",
    "            FN = CM[1][0]\n",
    "            TP = CM[1][1]\n",
    "            FP = CM[0][1]\n",
    "            sensitivity = TP / (TP + FN)\n",
    "            specificity = TN / (TN + FP)\n",
    "            PPV = TP / (TP + FP)\n",
    "            NPV = TN / (TN + FN)\n",
    "\n",
    "            test_predictions = alg.predict_proba(X_test)\n",
    "            ll = log_loss(Y_test, test_predictions)\n",
    "            print(\"\\t\\tLog Loss: {:.4}\".format(ll))\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = (end_time - start_time)\n",
    "            print(\"\\t\\tRuntime in seconds: {:.4}\".format(elapsed_time))\n",
    "\n",
    "            # Add the entry to the table\n",
    "            folds_entry = pd.DataFrame([[fold, name, rocauc * 100, acc * 100, balacc * 100, ll, sensitivity, specificity, PPV, NPV, elapsed_time]], columns=folds_cols)\n",
    "            folds_table = folds_table.append(folds_entry)\n",
    "\n",
    "            fold = fold + 1\n",
    "\n",
    "    folds_table.index = range(len(folds_table.index)) # Give to the index the correct names\n",
    "\n",
    "    # Get the best model for every Algorithm and add it to the final table\n",
    "    for alg in algsML:\n",
    "        name = alg.__class__.__name__\n",
    "        alg_best_entry = folds_table.iloc[folds_table[folds_table[\"Algorithm\"] == name]['Balanced_Accuracy_Percent'].idxmax()]\n",
    "        finalModels[name] = models[alg_best_entry[\"Fold\"]][name]\n",
    "        algs_table = algs_table.append(alg_best_entry)\n",
    "    \n",
    "    algs_table.index = range(len(algs_table.index))\n",
    "    print(\"\\n\\n\")\n",
    "    print(algs_table)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    # Save models in file\n",
    "    with open(workPath + \"models.pydat\", 'wb') as finalModels_file:\n",
    "        pickle.dump(finalModels, finalModels_file)\n",
    "\n",
    "    # We save the columns used in training for later\n",
    "    finalModels[\"columns\"] = X.columns\n",
    "\n",
    "    return finalModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TrainAndTestMML_1SmE function\n",
    "Function that creates the data for MetaML and obtains the level 2 expert of one repository in Meta Learning 1SmE\n",
    "\n",
    "Parameters:\n",
    "\n",
    "&emsp;__experts_L1__: Experts obtained from the function genModels<br>\n",
    "&emsp;__handlerMLdata__: Handler that contains the *.dataForML files with the data<br>\n",
    "&emsp;__workPath__: String with your workpath<br>\n",
    "\n",
    "Return value:\n",
    "\n",
    "&emsp;The model generated by training the metaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndTestMML_1SmE(experts_L1,\n",
    "                         handlerMLdata,\n",
    "                         workPath=\"/home/edusal/MetaLearning/metaML/Modelos/\"):\n",
    "\n",
    "    # Initialize variables\n",
    "    \n",
    "    # These are the algorithms we use in Meta-ML\n",
    "    algsMML = [RandomForestClassifier(), LogisticRegression(solver='lbfgs'), DecisionTreeClassifier()]\n",
    "\n",
    "    # These will be the columns for our final data table\n",
    "    algs_cols = [\"Fold\", \n",
    "                 \"Algorithm\", \n",
    "                 \"AUC_Percent\", \n",
    "                 \"Accuracy_Percent\", \n",
    "                 \"Balanced_Accuracy_Percent\", \n",
    "                 \"Log_Loss\", \n",
    "                 \"Sensitivity\", \n",
    "                 \"Specificity\", \n",
    "                 \"PPV\", \n",
    "                 \"NPV\", \n",
    "                 \"Runtime_Seconds\"]\n",
    "    algs_table = pd.DataFrame(columns=algs_cols)\n",
    "\n",
    "    # These will be the columns for our total data table (same columns as before)\n",
    "    folds_cols = algs_cols\n",
    "    folds_table = pd.DataFrame(columns=folds_cols)\n",
    "    \n",
    "    models = {}      # A dictionary with all models\n",
    "    finalModels = {} # A dictionary with final models\n",
    "    k = 5            # Number of folds\n",
    "\n",
    "    for i in range(1, k + 1):\n",
    "        models[i] = {}\n",
    "    \n",
    "    \n",
    "    # Prepare data for training\n",
    "    dataTest = pd.read_csv(handlerMLdata[\"test1mldata\"], header=0, delim_whitespace=True) # Read data\n",
    "    totalData = dataTest\n",
    "\n",
    "    totalData = totalData.loc[totalData[\"AGE\"] != -9] # Remove data with invalid age\n",
    "    ID = totalData[\"ID\"]\n",
    "    totalData.drop(\"ID\", 1, inplace=True) # We dont need id for training\n",
    "    \n",
    "    # Get the target variable\n",
    "    Y = totalData[\"PHENO\"]\n",
    "    totalData.drop(\"PHENO\", 1, inplace=True)\n",
    "\n",
    "    # Initialize metaSet with the target variable and ID\n",
    "    Y.index = range(len(Y.index))\n",
    "    ID.index = range(len(ID.index))\n",
    "\n",
    "    metaSet = pd.concat([ID, Y], axis=1)\n",
    "\n",
    "    # As an argument in future version\n",
    "    algsML = [RandomForestClassifier(), LogisticRegression(solver='lbfgs'), DecisionTreeClassifier()]\n",
    "\n",
    "    # Obtain the predictions for each Algorithm so we use them in the meta training\n",
    "    for alg in algsML:\n",
    "        name = alg.__class__.__name__\n",
    "        preds = experts_L1[name].predict(totalData)\n",
    "        metaSet[\"Pred-\" + name] = preds\n",
    "\n",
    "    # We assume MGSA type (we add age, sex and geno)\n",
    "    cols = totalData.columns\n",
    "    cols = [col for col in cols if not col.startswith(\"PC\")]\n",
    "    all_cols = totalData[cols]\n",
    "\n",
    "    metaSet.index = range(len(metaSet.index))\n",
    "    all_cols.index = range(len(all_cols.index))\n",
    "\n",
    "    metaSet = pd.concat([metaSet, all_cols], axis=1)\n",
    "\n",
    "    # We no longer need ID and Pheno in metaSet\n",
    "    ID = metaSet[\"ID\"]\n",
    "    metaSet.drop(\"ID\", 1, inplace=True)\n",
    "    metaSet.drop(\"PHENO\", 1, inplace=True)\n",
    "\n",
    "    # Separate the new dataframe into train and test (75% train - 25% test)\n",
    "    X_trainMML, X_testMML, Y_trainMML, Y_testMML = train_test_split(metaSet, Y, test_size=0.25)\n",
    "    X_np = X_trainMML.to_numpy() # We need our data in numpy format\n",
    "\n",
    "    # Process the target value\n",
    "    Y_np = Y_trainMML.to_numpy()\n",
    "    le = LabelEncoder()\n",
    "    le.fit(Y_np)\n",
    "    Y_le = le.transform(Y_np)\n",
    "    \n",
    "    # Make folds\n",
    "    np.random.seed(1234)\n",
    "    myfolds = KFold(n_splits=k)\n",
    "\n",
    "    for alg in algsMML: # For each algorithm\n",
    "\n",
    "        # Get the algorithm name and print it\n",
    "        name = alg.__class__.__name__\n",
    "\n",
    "        print()\n",
    "        print(\"#\" * 30)\n",
    "        print(name + \"\\n\")\n",
    "\n",
    "        fold = 1\n",
    "\n",
    "        for train_index, test_index in myfolds.split(X_np): # For each fold\n",
    "\n",
    "            print()\n",
    "            print(\"\\t\" + \"#\" * 26)\n",
    "            print(\"\\t\" + \"Fold \" + str(fold) + \"\\n\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            np.random.seed(1234)\n",
    "            \n",
    "            # Get the train and test sets\n",
    "            X_train, X_test = X_np[train_index], X_np[test_index]\n",
    "            Y_train, Y_test = Y_le[train_index], Y_le[test_index]\n",
    "\n",
    "            # Train\n",
    "            alg.fit(X_train, Y_train)\n",
    "\n",
    "            # Save the model\n",
    "            models[fold][name] = alg\n",
    "\n",
    "            test_predictions = alg.predict_proba(X_test)\n",
    "            rocauc = roc_auc_score(Y_test, test_predictions[:, 1])\n",
    "            print(\"\\t\\tAUC: {:.4%}\".format(rocauc))\n",
    "\n",
    "            test_predictions = alg.predict(X_test)\n",
    "            acc = accuracy_score(Y_test, test_predictions)\n",
    "            print(\"\\t\\tAccuracy: {:.4%}\".format(acc))\n",
    "\n",
    "            test_predictions = alg.predict(X_test)\n",
    "            balacc = balanced_accuracy_score(Y_test, test_predictions)\n",
    "            print(\"\\t\\tBalanced Accuracy: {:.4%}\".format(balacc))\n",
    "\n",
    "            test_predictions = alg.predict(X_test)\n",
    "            kappa = cohen_kappa_score(Y_test, test_predictions)\n",
    "            print(\"\\t\\tKappa: {:.4%}\".format(kappa))\n",
    "\n",
    "            CM = confusion_matrix(Y_test, test_predictions)\n",
    "            TN = CM[0][0]\n",
    "            FN = CM[1][0]\n",
    "            TP = CM[1][1]\n",
    "            FP = CM[0][1]\n",
    "            sensitivity = TP / (TP + FN)\n",
    "            specificity = TN / (TN + FP)\n",
    "            PPV = TP / (TP + FP)\n",
    "            NPV = TN / (TN + FN)\n",
    "\n",
    "            test_predictions = alg.predict_proba(X_test)\n",
    "            ll = log_loss(Y_test, test_predictions)\n",
    "            print(\"\\t\\tLog Loss: {:.4}\".format(ll))\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = (end_time - start_time)\n",
    "            print(\"\\t\\tRuntime in seconds: {:.4}\".format(elapsed_time))\n",
    "\n",
    "            # Add the entry to the table\n",
    "            folds_entry = pd.DataFrame([[fold, name, rocauc * 100, acc * 100, balacc * 100, ll, sensitivity, specificity, PPV, NPV, elapsed_time]], columns=algs_cols)\n",
    "            folds_table = folds_table.append(folds_entry)\n",
    "\n",
    "            fold = fold + 1\n",
    "\n",
    "    # Save models\n",
    "    with open(workPath + \"models_MML.pydat\", 'wb') as finalModels_file:\n",
    "        pickle.dump(finalModels, finalModels_file)\n",
    "\n",
    "    folds_table.index = range(len(folds_table.index))\n",
    "\n",
    "    # Get the best model for every Algorithm and add it to the final table\n",
    "    for alg in algsMML:\n",
    "        name = alg.__class__.__name__\n",
    "        alg_best_entry = folds_table.iloc[folds_table[folds_table[\"Algorithm\"] == name]['Balanced_Accuracy_Percent'].idxmax()]\n",
    "        finalModels[name] = models[alg_best_entry[\"Fold\"]][name]\n",
    "        algs_table = algs_table.append(alg_best_entry)\n",
    "\n",
    "    algs_table.index = range(len(algs_table.index))\n",
    "    print(\"\\n\\n\")\n",
    "    print(algs_table)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Get the best model\n",
    "    bestAlg = algs_table.iloc[algs_table['Balanced_Accuracy_Percent'].idxmax()]['Algorithm']\n",
    "\n",
    "    # Create the return variable with the best model and the columns used in metatraining\n",
    "    expert_L2 = {}\n",
    "    expert_L2[\"model\"] = finalModels[bestAlg]\n",
    "    expert_L2[\"columns\"] = metaSet.columns\n",
    "    metaSet[\"ID\"] = ID # Add ID to the metaSet\n",
    "    \n",
    "    # Save dataframes and results\n",
    "    with open(workPath + \"metaSet.pydat\", 'wb') as metaSet_file:\n",
    "        pickle.dump(metaSet, metaSet_file)\n",
    "\n",
    "    with open(workPath + \"expert_L2.pydat\", 'wb') as expert_L2_file:\n",
    "        pickle.dump(expert_L2, expert_L2_file)\n",
    "\n",
    "    with open(workPath + \"metaResults.pydat\", 'wb') as metaResults_file:\n",
    "        pickle.dump(folds_table, metaResults_file)\n",
    "\n",
    "    return expert_L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrepareFinalTest function\n",
    "Function that generates the ML data for the repo reserved for the test part\n",
    "\n",
    "Parameters:\n",
    "\n",
    "&emsp;__workPath__: String with your workpath<br>\n",
    "&emsp;__path2Geno__: Path to your genotype data<br>\n",
    "&emsp;__path2Covs__:Path to your covariates data<br>\n",
    "&emsp;__predictor__: What you want to predict<br>\n",
    "&emsp;__addit__: <br>\n",
    "&emsp;__snpsToPull__: SNPs selected from the spanish repo, to use them to extract variables in the testing repo<br>\n",
    "&emsp;__path2plink__: Path to plink<br>\n",
    "\n",
    "Return value:\n",
    "\n",
    "&emsp;Data prepare for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareFinalTest(workPath,\n",
    "                     path2Geno,\n",
    "                     path2Covs,\n",
    "                     predictor,\n",
    "                     snpsToPull,\n",
    "                     path2plink=\"/home/edusal/packages/\",\n",
    "                     addit=\"NA\"):\n",
    "\n",
    "    command = \"cp \" + path2Covs + \".cov \" + workPath\n",
    "    print(\"The command to run: \" + command)\n",
    "    os.system(command)\n",
    "\n",
    "    # Get the repo's handler\n",
    "    handler = fm.getHandlerToGenotypeData(geno=path2Geno,\n",
    "                                          covs=path2Covs,\n",
    "                                          id=\"IID\",\n",
    "                                          fid=\"FID\",\n",
    "                                          predictor=predictor,\n",
    "                                          pheno=workPath + \"/MyPhenotype\")\n",
    "\n",
    "    geno = os.path.basename(handler[\"geno\"])\n",
    "    pheno = os.path.basename(handler[\"pheno\"])\n",
    "    cov = os.path.basename(handler[\"covs\"])\n",
    "    path2Genotype = os.path.dirname(handler[\"geno\"]) + \"/\"\n",
    "    prefix = \"g-\" + geno + \"-p-\" + pheno + \"-c-\" + cov + \"-a-\" + addit\n",
    "    fprefix = workPath + \"/\" + prefix\n",
    "    \n",
    "    # Set the SNPs to pull to the ones selected in the spanish set\n",
    "    handler[\"snpsToPull\"] = snpsToPull  \n",
    "\n",
    "    # Run Plink\n",
    "    command = path2plink + \"plink --bfile \" + path2Genotype + geno + \" --extract \" +\\\n",
    "        handler[\"snpsToPull\"] + \" --recode A --out \" + fprefix + \".reduced_genos\"\n",
    "    print(\"Running command \" + command + \"\\n\")\n",
    "    os.system(command)\n",
    "\n",
    "    # Exports SNP list for extraction in validation set\n",
    "    command = \"cut -f 1 \" + handler[\"snpsToPull\"] + \" > \" + fprefix + \".reduced_genos_snpList\"\n",
    "    print(\"Running command \" + command + \"\\n\")\n",
    "    os.system(command)\n",
    "\n",
    "    handler[\"rgenosSnpList\"] = fprefix + \".reduced_genos_snpList\"\n",
    "\n",
    "    # Generate the .dataForML file\n",
    "    mldatahandler = fm.fromSNPs2MLdata(handler=handler,\n",
    "                                       addit=\"NA\",\n",
    "                                       path2plink=path2plink,\n",
    "                                       predictor=predictor)\n",
    "\n",
    "    # Save it\n",
    "    with open(workPath + \"mldatahandler.pydat\", 'wb') as mldatahandler_file:\n",
    "        pickle.dump(mldatahandler, mldatahandler_file)\n",
    "\n",
    "    return mldatahandler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinalTest_1SmE function\n",
    "Function that obtains the prediction in the test repository in the Meta Learning 1SmE.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "&emsp;__workPath__: String with your workpath<br>\n",
    "&emsp;__expert_L2__: Expert returned by the MML 1SmE<br>\n",
    "&emsp;__handlerTest__: Handler with the mldata of the test set<br>\n",
    "&emsp;__experts_L1__: Experts used in the generation of the metaset<br>\n",
    "\n",
    "Return value:\n",
    "\n",
    "&emsp;Results of training with with test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalTest_1SmE(workPath,\n",
    "                   expert_L2,\n",
    "                   handlerTest,\n",
    "                   experts_L1):\n",
    "\n",
    "    dataFinal = pd.read_csv(handlerTest[\"train1mldata\"], header=0, delim_whitespace=True) # Read data\n",
    "\n",
    "    # same preprocess applied to the training\n",
    "    ID = dataFinal[\"ID\"]\n",
    "    dataFinal.drop(\"ID\", 1, inplace=True)\n",
    "\n",
    "    Y = dataFinal[\"PHENO\"]\n",
    "    dataFinal.drop(\"PHENO\", 1, inplace=True)\n",
    "\n",
    "    Y_np = Y.to_numpy()\n",
    "    le = LabelEncoder()\n",
    "    le.fit(Y_np)\n",
    "    Y_le = le.transform(Y_np)\n",
    "\n",
    "    Y.index = range(len(Y.index))\n",
    "    ID.index = range(len(ID.index))\n",
    "\n",
    "    # dataframe which will contain the predictions from each model\n",
    "    metaSet = pd.concat([ID, Y], axis=1)\n",
    "\n",
    "    aux = dataFinal\n",
    "\n",
    "    # Add variables set to zero, present in each expert but not in the test data\n",
    "    diff1 = set(experts_L1[\"columns\"]) - set(aux.columns)\n",
    "    if len(diff1) > 0:\n",
    "        new_cols = pd.DataFrame(0, index=np.arange(len(aux)), columns=diff1)\n",
    "\n",
    "        aux.index = range(len(aux.index))\n",
    "        aux = pd.concat([aux, new_cols], axis=1)\n",
    "    \n",
    "    # Remove variables in the test data and not in the data used for training each expert\n",
    "    diff2 = set(aux.columns) - set(experts_L1[\"columns\"])\n",
    "    if len(diff2) > 0:\n",
    "        aux.drop(diff2, 1, inplace=True)\n",
    "\n",
    "    # As argument in future version\n",
    "    algsML = [RandomForestClassifier(), LogisticRegression(solver='lbfgs'), DecisionTreeClassifier()]\n",
    "    \n",
    "    # Obtain the predictions for each Algorithm so we use them in the meta training\n",
    "    for alg in algsML:\n",
    "        name = alg.__class__.__name__\n",
    "\n",
    "        preds = experts_L1[name].predict(aux)\n",
    "        metaSet[\"Pred-\" + name] = preds\n",
    "\n",
    "    # We assume its type MGSA and add age, sex and geno\n",
    "    cols = dataFinal.columns\n",
    "    cols = [col for col in cols if not col.startswith(\"PC\")]\n",
    "    all_cols = dataFinal[cols]\n",
    "\n",
    "    metaSet.index = range(len(metaSet.index))\n",
    "    all_cols.index = range(len(all_cols.index))\n",
    "\n",
    "    metaSet = pd.concat([metaSet, all_cols], axis=1)\n",
    "\n",
    "    ID = metaSet[\"ID\"]\n",
    "    metaSet.drop(\"ID\", 1, inplace=True)\n",
    "    metaSet.drop(\"PHENO\", 1, inplace=True)\n",
    "\n",
    "    aux = metaSet\n",
    "    \n",
    "    # add variables set to zero present in the MML model but not in the data\n",
    "    diff1 = set(expert_L2[\"columns\"]) - set(aux.columns)\n",
    "    if len(diff1) > 0:\n",
    "        new_cols = pd.DataFrame(0, index=np.arange(len(aux)), columns=diff1)\n",
    "\n",
    "        aux.index = range(len(aux.index))\n",
    "        aux = pd.concat([aux, new_cols], axis=1)\n",
    "\n",
    "    # remove variables in the metaset and not in the data used for training the MML model\n",
    "    diff2 = set(aux.columns) - set(expert_L2[\"columns\"])\n",
    "    if len(diff2) > 0:\n",
    "        aux.drop(diff2, 1, inplace=True)\n",
    "\n",
    "    # Put the columns in the right order\n",
    "    metaSet = aux[expert_L2[\"columns\"].tolist()]\n",
    "\n",
    "    # Save metaSet\n",
    "    with open(workPath + \"metaSet.pydat\", 'wb') as metaSet_file:\n",
    "        pickle.dump(metaSet, metaSet_file)\n",
    "\n",
    "    # Get final results\n",
    "    test_predictions = expert_L2[\"model\"].predict_proba(metaSet)\n",
    "    rocauc = roc_auc_score(Y_le, test_predictions[:, 1])\n",
    "    print(\"\\t\\tAUC: {:.4%}\".format(rocauc))\n",
    "\n",
    "    test_predictions = expert_L2[\"model\"].predict(metaSet)\n",
    "    acc = accuracy_score(Y_le, test_predictions)\n",
    "    print(\"\\t\\tAccuracy: {:.4%}\".format(acc))\n",
    "\n",
    "    test_predictions = expert_L2[\"model\"].predict(metaSet)\n",
    "    balacc = balanced_accuracy_score(Y_le, test_predictions)\n",
    "    print(\"\\t\\tBalanced Accuracy: {:.4%}\".format(balacc))\n",
    "\n",
    "    test_predictions = expert_L2[\"model\"].predict(metaSet)\n",
    "    kappa = cohen_kappa_score(Y_le, test_predictions)\n",
    "    print(\"\\t\\tKappa: {:.4%}\".format(kappa))\n",
    "\n",
    "    CM = confusion_matrix(Y_le, test_predictions)\n",
    "\n",
    "    test_predictions = expert_L2[\"model\"].predict_proba(metaSet)\n",
    "    ll = log_loss(Y_le, test_predictions)\n",
    "    print(\"\\t\\tLog Loss: {:.4}\".format(ll))\n",
    "\n",
    "    # Save final results\n",
    "    finalResults = {}\n",
    "    finalResults[\"PHENO\"] = Y\n",
    "    finalResults[\"AUC\"] = rocauc\n",
    "    finalResults[\"Accuracy\"] = acc\n",
    "    finalResults[\"Balanced Accuracy\"] = balacc\n",
    "    finalResults[\"Kappa\"] = kappa\n",
    "\n",
    "    finalResults[\"confMat\"] = CM\n",
    "\n",
    "    return(finalResults)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
